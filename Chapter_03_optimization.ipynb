{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 3\n",
    "## Learning as optimization\n",
    "\n",
    "\n",
    "    Hand-in bug-free (try \"Kernel\" > \"Restart & Run All\") and including all (textual as well as figural) output via Blackboard before the deadline (see Blackboard).\n",
    "    \n",
    "Learning goals:\n",
    "1. get familiar with partial derivatives\n",
    "1. get familiar with gradient descent\n",
    "1. get familiar with the cross-entropy loss function\n",
    "1. get familiar with optimizing a one-layer neural network with gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Exercise 1: A simple parabola (0.25 points)\n",
    "Consider a simple parabola: $y=x^2$. Write a function `f(x)` that computes and returns $y$ given any $x$. Additionally, write a function `df(x)` that computes and returns the derivative of this parabola at any $x$. \n",
    "\n",
    "We will use these functions to find the minimum of $y$ with gradient descent. Of course, one could do so by setting the derivative to zero and solving the problem analytically. Note however, that in real-world cases (e.g., for complex neural networks), we usually can not find a closed form solution for finding the minimum of the derivative, analytically. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    ###  Add your code here. ###\n",
    "    return y\n",
    "\n",
    "def df(x):\n",
    "    ###  Add your code here. ###\n",
    "    return dy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Gradient descent (1 point)\n",
    "Write a function `gradient_descent(function,derivative,x0,eta,n)` that performs gradient descent on $y=x^2$ from a given starting x position $x_0$. It should use the given learning rate $\\eta$ and perform $n$ steps. Note that gradient descent has a very simple formula, based on the previous $x$ position. The function should save and return the trajectories for $x$ and $y$, as in the next exercise we want to observe what gradient descent does for different parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gradient_descent(function, derivative, x0, eta, n):\n",
    "    ###  Add your code here. ###\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Very low learning rate (0.25 points)\n",
    "We want to show the gradient descent trajectories for different learning rates. Create a plot that shows the parabola $y=x^2$ and a marker for the starting point of gradient descent $(x_0, f(x_0))$. We use $x_0=-7.5$ and $n=50$. Plot $(x_t,f(x_t))$ at each step (i.e. the returned trajectories) for $\\eta=0.001$. What do you observe? Is this a good learning rate?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Run gradient descent, get gradient descent trajectory x and y\n",
    "###  Add your code here. ###\n",
    "\n",
    "# Plot parabola f(x), starting point, gradient descent trajectory (x vs. y)\n",
    "###  Add your code here. ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4: Low learning rate (0.25 points)\n",
    "We want to show the gradient descent trajectories for different learning rates. Create a plot that shows the parabola $y=x^2$ and a marker for the starting point of gradient descent $(x_0, f(x_0))$. We use $x_0=-7.5$ and $n=50$. Plot $(x_t,f(x_t))$ at each step (i.e. the returned trajectories) for $\\eta=0.05$. What do you observe? Is this a good learning rate?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Run gradient descent, get gradient descent trajectory x and y\n",
    "###  Add your code here. ###\n",
    "\n",
    "# Plot parabola f(x), starting point, gradient descent trajectory (x vs. y)\n",
    "###  Add your code here. ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5: High learning rate (0.25 points)\n",
    "We want to show the gradient descent trajectories for different learning rates. Create a plot that shows the parabola $y=x^2$ and a marker for the starting point of gradient descent $(x_0, f(x_0))$. We use $x_0=-7.5$ and $n=50$. Plot $(x_t,f(x_t))$ at each step (i.e. the returned trajectories) for $\\eta=0.5$ What do you observe? Is this a good learning rate?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Run gradient descent, get gradient descent trajectory x and y\n",
    "###  Add your code here. ###\n",
    "\n",
    "# Plot parabola f(x), starting point, gradient descent trajectory (x vs. y)\n",
    "###  Add your code here. ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6: Very high learning rate (0.25 points)\n",
    "We want to show the gradient descent trajectories for different learning rates. Create a plot that shows the parabola $y=x^2$ and a marker for the starting point of gradient descent $(x_0, f(x_0))$. We use $x_0=-7.5$ and $n=50$. Plot $(x_t,f(x_t))$ at each step (i.e. the returned trajectories) for $\\eta=1.01$. What do you observe? Is this a good learning rate?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Run gradient descent, get gradient descent trajectory x and y\n",
    "###  Add your code here. ###\n",
    "\n",
    "# Plot parabola f(x), starting point, gradient descent trajectory (x vs. y)\n",
    "###  Add your code here. ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7: Cross-entropy loss (1 point)\n",
    "The cross-entropy loss is a common loss function used for classification, and is given by:\n",
    "$$L = \\frac{1}{N} \\sum^N - t^{(n)}\\log(y^{(n)}) - (1 - t^{(n)})\\log(1 - y^{(n)})$$\n",
    "Here, $t^{(n)}$ is the target (real value) which is either 0 or 1, and $y^{(n)}$ the prediction of the $n$th example is a continues value between 0 and 1 (output of a sigmoid activation).\n",
    "\n",
    "The cross-entropy loss is low (towards zero) when the MLP is precisely right (true negatives, true positives). It is high (towards infinity) when the MLP is all wrong (false negatives, false positives). Read and understand the function to verify that this is indeed the case (i.e., what happens when $y\\neq t$ and when $y=t$). \n",
    "\n",
    "We need the derivative of the cross-entropy loss function with respect to $y$ to do gradient descent (i.e., backpropagation) to tweak the parameters (i.e., the weights) for classification. Derive the cross-entropy loss function with respect to $y$, i.e. derive $\\frac{\\partial L}{\\partial y}$. You can ignore the sum over the batch; use: $L = -t \\log(y) - (1 - t)\\log(1 - y)$. Simplify the derivation as much as possible (i.e. to one fraction of two terms). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 7:\n",
    "Write $\\LaTeX$ here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 8: The sigmoid activation function (1 point)\n",
    "The sigmoid activation function is an activation function that squeezes its input into the range of 0 and 1. This is convenient, especially as $y$ at the output, and also somewhat interpretable as a probability for a certain class. The sigmoid is given by:\n",
    "\n",
    "$$f(a) = \\frac{1}{1+\\exp(-a)}$$\n",
    "\n",
    "Derive the derivative of the sigmoid activation function with respect to the activation $a$, and show that the derivative is equal to a combination of two sigmoids: \n",
    "\n",
    "$$\\frac{\\partial f(a)}{\\partial a} = \\frac{\\partial y}{\\partial a} = f(a)(1 - f(a))$$\n",
    "\n",
    "This simple derivative is another convenient property of the sigmoid activation function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 8\n",
    "Write $\\LaTeX$ here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 9: The forward pass (0.5 points)\n",
    "We are going to build a one-layer network to classify digits. The inputs $x$ are images of handwritten digits. Here, we restrict the problem to binary classification, so we classify only two possible digits (0 and 1). This means that we need only one a single output unit $y$. \n",
    "\n",
    "In the forward pass the input $x$ is weighted by one layer of weights $w$. Then this activation $a$ is passed on into the sigmoid unit activation function, producing $y$. \n",
    "\n",
    "Write down the equations for the activation $a$ and the output $y$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 9:\n",
    "Add $\\LaTeX$ here.\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "a &=&  \\\\\n",
    "y &=&  \\\\\n",
    "\\end{eqnarray*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 10: The backward pass (1 point)\n",
    "\n",
    "To update the weights so that the loss is further reduced (i.e. to gradually let the network learn to do the right thing), we need the partial derivatives of the weights. To compute the partial derivatives of the weights $w$, we have to propagate from the error function back through the activation function to the weights. Note that we do not have to handle any hidden units and their activations in this one-layer network. \n",
    "\n",
    "Obtain $\\frac{\\partial L}{\\partial w}$ by applying the chain rule multiple times. Note that $L$ is the cross-entropy loss, that you have already derived $\\frac{\\partial L}{\\partial y}$ and $\\frac{\\partial y}{\\partial a}$ further above, and that $\\frac{\\partial a}{\\partial w}$ has a trivial derivative. Simplify $\\frac{\\partial L}{\\partial w}$ as much as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 10:\n",
    "Write $\\LaTeX$ here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 11: Implementation (2.25 points)\n",
    "Now that all the math is done, we can start implementing the one-layer network for binary classification of two digits, where we make use of sigmoid units and the cross-entropy loss. Write the following functions:\n",
    "1. `cross_entropy(Y, T)`: Computes the cross entropy loss. Make sure that there are no exact-zero inputs for `np.log()` (something simple like adding a very small number if `Y` is 0 is fine). \n",
    "1. `sigmoid(A)`: Passes the activity `A` through the sigmoid activation function.\n",
    "1. `linear(X, W)`: Computes the activities `A` as `X` weighted by `W`.\n",
    "1. `forward(X, W)`: Computes the forward pass for the one-layer network with a sigmoid output unit. Returns `Y`. \n",
    "1. `backward(X, Y, T)`: Computes the backward pass for the one-layer network with a sigmoid output unit and cross-entropy loss.\n",
    "1. `train_network(X_train, T_train, X_val, T_val, n_epochs, eta)`: Implement the training procedure (learn the weights). See the skeleton code for some help.\n",
    "1. `test_network(X, W)`: Predicts new examples given as `X`, returns classes as a binary label vector (for classes 0 and 1). Interpret the output of the network (the sigmoid) as a probability for class 1 and transform these probabilities to this binary label vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cross_entropy(Y, T):\n",
    "    \"\"\"\n",
    "    Computes the cross-entropy loss.\n",
    "    INPUT:\n",
    "        Y = [1 N] output vector for N examples\n",
    "        T = [1 N] tagret vector for N examples\n",
    "    OUTPUTS\n",
    "        L = [1 1] the mean cross-entropy loss\n",
    "    \"\"\"\n",
    "    ###  Add your code here. ###\n",
    "    return L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(A):\n",
    "    \"\"\"\n",
    "    Computes the sigmoid activation function.\n",
    "    INPUT:\n",
    "        A = [K N] activity matrix of K units for N examples\n",
    "    OUTPUT\n",
    "        Y = [K N] output matrix of K units for N examples\n",
    "    \"\"\"\n",
    "    ###  Add your code here. ###\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def linear(X, W):\n",
    "    \"\"\"\n",
    "    Computes the activities for a fully connected layer.\n",
    "    INPUT:\n",
    "        X = [P N] data matrix of P input units for N examples\n",
    "        W = [Q P] weight matrix of P inputs to Q outputs\n",
    "    OUTPUT\n",
    "        A = [Q N] activity matrix of Q output units for N examples\n",
    "    \"\"\"\n",
    "    ###  Add your code here. ###\n",
    "    return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def forward(X, W):\n",
    "    \"\"\"\n",
    "    Computes the forward pass for a one-layer network with sigmoid units.\n",
    "    INPUT\n",
    "        X = [P N] data matrix of P inputs for N examples\n",
    "        W = [1 P] weight matrix of the first layer of P inputs to 1 output\n",
    "    OUTPUT\n",
    "        Y = [1 N] output vector for N examples\n",
    "    \"\"\"\n",
    "    ###  Add your code here. ###\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def backward(X, Y, T):\n",
    "    \"\"\"\n",
    "    Computes the backward pass for a one-layer network with sigmoid units and cross-entropy loss.\n",
    "    INPUT:\n",
    "        X  = [P N] data matrix of P inputs for N examples\n",
    "        Y  = [1 N] a vector of predictions for N examples\n",
    "        T  = [1 N] a vector of targets for N examples\n",
    "    OUTPUT\n",
    "        dW = [1 P] gradient matrix for the weights of P inputs to 1 output\n",
    "    \"\"\"\n",
    "    ###  Add your code here. ###\n",
    "    return dW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_network(X_train, T_train, X_val, T_val, n_epochs=100, eta=0.001):\n",
    "    \"\"\"\n",
    "    Performs the training procedure for a one-layer network with sigmoid units and cross-entropy loss.\n",
    "    INPUT:\n",
    "        X_train  = [P N] data matrix of P inputs for N training examples\n",
    "        T_train  = [1 N] a vector of targets for N training examples \n",
    "                   (labels for digits 0 or 1; given as numbers 0 or 1)\n",
    "        X_val    = [P M] data matrix of P inputs for N training examples\n",
    "        T_val    = [1 M] a vector of targets for N training examples\n",
    "        n_epochs = [1 1] number of training epochs (default 100)\n",
    "        eta      = [1 1] learning rate (default 0.001)\n",
    "    OUTPUT:\n",
    "        W          = [1 P] the learned weights of P inputs to 1 outputs\n",
    "        train_loss = [Z 1] the training loss for Z epochs\n",
    "        val_loss   = [Z 1] the validation loss for Z epochs\n",
    "    \"\"\"\n",
    "    \n",
    "    # Weight initialization\n",
    "    W = np.random.randn(1, X.shape[0])\n",
    "    \n",
    "    # Arrays for saving losses\n",
    "    train_loss = np.zeros((n_epochs))\n",
    "    val_loss = np.zeros((n_epochs))\n",
    "\n",
    "    # Loop over epochs\n",
    "    for i_epoch in xrange(n_epochs):\n",
    "        \n",
    "        # Forward pass for current network (i.e. with current W)\n",
    "        # (obtaining Y_train)\n",
    "        ###  Add your code here. ###\n",
    "        \n",
    "        # Backward pass\n",
    "        # (obtaining dW)\n",
    "        ###  Add your code here. ###\n",
    "        \n",
    "        # Parameter update\n",
    "        # (adjusting W with given learning rate)\n",
    "        ###  Add your code here. ###\n",
    "        \n",
    "        # Save losses for train and validation set (compute with cross entropy loss function)\n",
    "        ###  Add your code here. ###\n",
    "        \n",
    "        # Print progress and loss\n",
    "        if i_epoch % 10 == 0:\n",
    "            print(\"Epoch {}/{}. Train loss: {}. Validation loss: {}.\".format(\n",
    "                1+i_epoch, n_epochs, train_loss[i_epoch], val_loss[i_epoch]))\n",
    "        \n",
    "    return W, train_loss, val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_network(X, W):\n",
    "    \"\"\"\n",
    "    Applies the trained one-layer network with sigmoid units to classify data.\n",
    "    INPUT:\n",
    "        X = [P N] data matrix of P inputs for N examples\n",
    "        W = [1 P] weight matrix of P inputs to 1 output\n",
    "    OUTPUT\n",
    "        Y = [1 N] output vector (i.e., predicted labels) for N examples\n",
    "    \"\"\"\n",
    "    ###  Add your code here. ###\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 12: Training (1 point)\n",
    "Now that we have done all the work, we can finally run the network. Below we first load in the digit dataset, and restrict it to two digits. We split this data into a training and a test set. \n",
    "\n",
    "Train your network on the training dataset `X_train` and `T_train`, and validate it at each epoch on the test set `X_val` and `T_val`. After training, plot the train and validation losses over epochs (as returned by `train_network()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read full dataset from mat file\n",
    "mat = sio.loadmat(\"digits.mat\")\n",
    "X = mat[\"digits\"]\n",
    "T = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 0]).repeat(1000)\n",
    "sz = (28, 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Select only digits 0 and 1 for binary classification\n",
    "digits = np.array([0, 1])\n",
    "idx = np.in1d(T, digits)\n",
    "X = X[:, idx]\n",
    "T = np.array([0, 1]).repeat(1000)  # class labels given as 0 and 1 then"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plot an example of both classes\n",
    "fig, ax = plt.subplots(1, 2)\n",
    "ax[0].imshow(X[:, 0].reshape(sz).T, cmap=\"gray\")\n",
    "ax[1].imshow(X[:, 1000].reshape(sz).T, cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split dataset in training, validation, and testing split\n",
    "X_train, X_test, T_train, T_test = train_test_split(X.T, T, test_size=0.2)\n",
    "X_train, X_val, T_train, T_val = train_test_split(X_train, T_train, test_size=0.2)\n",
    "\n",
    "# Transpose back\n",
    "X_train = X_train.T\n",
    "X_val = X_val.T\n",
    "X_test = X_test.T\n",
    "\n",
    "T_train = T_train.reshape((1, T_train.size))\n",
    "T_val = T_val.reshape((1, T_val.size))\n",
    "T_test = T_test.reshape((1, T_test.size))\n",
    "\n",
    "\n",
    "# Print dimensions\n",
    "print(\"Dimensions training inputs: {}, and training outputs: {}\".format(X_train.shape, T_train.shape))\n",
    "print(\"Dimensions validation inputs: {}, and validation outputs: {}\".format(X_val.shape, T_val.shape))\n",
    "print(\"Dimensions testing inputs: {}, and testing outputs: {}\".format(X_test.shape, T_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Train network\n",
    "###  Add your code here. ###\n",
    "\n",
    "# Plot losses\n",
    "###  Add your code here. ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 13: Testing (1 point)\n",
    "Now that the network is trained, we can obtain a test score on a held out test set, and compute a classification performance. Apply your network to the test set, and print its accuracy. \n",
    "\n",
    "Note that if you don't avoid looking at another separate test set during training your network you may accidentally overfit on your validation set, e.g. by hyperparameter choices. This is a common problem in machine learning literature.\n",
    "\n",
    "Also note, by implementing this specific type of architecture for neural networks (i.e., one weight layer, one output, sirgmoid activation function), you have actually implemented logistic regression!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Test network\n",
    "###  Add your code here. ###\n",
    "\n",
    "# Print accuracy\n",
    "###  Add your code here. ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
